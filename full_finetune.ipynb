{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWmRbVziYtai"
   },
   "source": [
    "# Full finetuning\n",
    "\n",
    "model: EleutherAI/pythia-70m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_xhX27MAZeIg"
   },
   "outputs": [],
   "source": [
    "!pip -q install transformers[torch] datasets\n",
    "# !pip -q install accelerate bitsandbytes sentencepiece Xformers peft trl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "V7vK3P3zYaRG"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import datasets\n",
    "import yaml\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2S3R4Isjc6BY",
    "outputId": "131beb67-4438-4546-cf7c-752bb1cb65e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device_count = torch.cuda.device_count()\n",
    "if device_count > 0:\n",
    "    logger.debug(\"Select GPU device\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    logger.debug(\"Select CPU device\")\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_split_data(training_config, tokenizer):\n",
    "    \"\"\"Wrapper for data load, split, tokenize for training.\"\"\"\n",
    "    initialized_config = training_config\n",
    "    dataset_path = training_config[\"datasets\"][\"path\"]\n",
    "    use_hf = training_config[\"datasets\"][\"use_hf\"]\n",
    "    print(\"tokenize\", use_hf, dataset_path)\n",
    "    if use_hf:\n",
    "        dataset = datasets.load_dataset(dataset_path)\n",
    "    else:\n",
    "        dataset = load_dataset(training_config, dataset_path, tokenizer)\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    test_dataset = dataset[\"test\"]\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def load_dataset(training_config, dataset_path, tokenizer):\n",
    "    \"\"\"Tokenize and split data.\"\"\"\n",
    "    random.seed(42)\n",
    "    finetuning_dataset_loaded = datasets.load_dataset(\n",
    "        \"json\", data_files=dataset_path, split=\"train\"\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    max_length = training_config[\"model\"][\"max_length\"]\n",
    "    tokenized_dataset = finetuning_dataset_loaded.map(\n",
    "        get_tokenize_function(tokenizer, max_length),  # returns tokenize_function\n",
    "        batched=True,\n",
    "        batch_size=1,\n",
    "        drop_last_batch=True,\n",
    "    )\n",
    "    tokenized_dataset = tokenized_dataset.with_format(\"torch\")\n",
    "    split_dataset = tokenized_dataset.train_test_split(\n",
    "        test_size=0.1, shuffle=True, seed=123\n",
    "    )\n",
    "    return split_dataset\n",
    "\n",
    "\n",
    "def get_tokenize_function(tokenizer, _max_length):\n",
    "    def tokenize_function(examples):\n",
    "        max_length = _max_length\n",
    "\n",
    "        # Set pad token\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        if \"question\" in examples and \"answer\" in examples:\n",
    "            text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "        elif \"input\" in examples and \"output\" in examples:\n",
    "            text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "        else:\n",
    "            text = examples[\"text\"][0]\n",
    "\n",
    "        # Run tokenizer on all the text (the input and the output)\n",
    "        tokenized_inputs = tokenizer(\n",
    "            text,\n",
    "            # Return tensors in a numpy array (other options are pytorch or tf objects)\n",
    "            return_tensors=\"np\",\n",
    "            # Padding type is to pad to the longest sequence in the batch (other option is to a certain max length, or no padding)\n",
    "            padding=True,\n",
    "        )\n",
    "\n",
    "        # Calculate max length\n",
    "        max_length = min(tokenized_inputs[\"input_ids\"].shape[1], max_length)\n",
    "\n",
    "        if tokenized_inputs[\"input_ids\"].shape[1] > max_length:\n",
    "            logger.warn(\n",
    "                f\"Truncating input from {tokenized_inputs['input_ids'].shape[1]} to {max_length}\"\n",
    "            )\n",
    "\n",
    "        tokenizer.truncation_side = \"left\"\n",
    "\n",
    "        tokenized_inputs = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"np\",\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"]\n",
    "\n",
    "        return tokenized_inputs\n",
    "\n",
    "    return tokenize_function\n",
    "\n",
    "\n",
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "    \"\"\"Inference.\"\"\"\n",
    "    # Tokenize\n",
    "    input_ids = tokenizer.encode(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_tokens\n",
    "    )\n",
    "\n",
    "    # Generate\n",
    "    device = model.device\n",
    "    generated_tokens_with_prompt = model.generate(\n",
    "        input_ids=input_ids.to(device),\n",
    "        max_length=max_output_tokens\n",
    "    )\n",
    "\n",
    "    # Decode\n",
    "    generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "    # Strip the prompt\n",
    "    generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "    return generated_text_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_0iBx6_wZLmQ"
   },
   "outputs": [],
   "source": [
    "dataset_name = \"lamini_docs.jsonl\"\n",
    "dataset_path = f\"./{dataset_name}\"\n",
    "use_hf = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fJ6MvOaCZQjQ"
   },
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/pythia-70m\"\n",
    "\n",
    "training_config = {\n",
    "    \"model\": {\n",
    "        \"pretrained_name\": model_name,\n",
    "        \"max_length\" : 2048\n",
    "    },\n",
    "    \"datasets\": {\n",
    "        \"use_hf\": use_hf,\n",
    "        \"path\": dataset_path\n",
    "    },\n",
    "    \"verbose\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "HB-AbUfRZW2Z",
    "outputId": "2e15dc61-4c20-4242-87ad-bbdd05820197",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize False ./lamini_docs.jsonl\n",
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1260\n",
      "})\n",
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 140\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GFc4niAxc3Em"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Gzh7u4ydDky"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Q7yvMp3mdtNz"
   },
   "outputs": [],
   "source": [
    "# Trainer class to include logging and history\n",
    "class Trainer(transformers.Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        model_flops,\n",
    "        total_steps,\n",
    "        args=None,\n",
    "        data_collator=None,\n",
    "        train_dataset=None,\n",
    "        eval_dataset=None,\n",
    "        tokenizer=None,\n",
    "        model_init=None,\n",
    "        compute_metrics=None,\n",
    "        callbacks=None,\n",
    "        optimizers=(None, None),\n",
    "    ):\n",
    "        super(Trainer, self).__init__(\n",
    "            model,\n",
    "            args,\n",
    "            data_collator,\n",
    "            train_dataset,\n",
    "            eval_dataset,\n",
    "            tokenizer,\n",
    "            model_init,\n",
    "            compute_metrics,\n",
    "            callbacks,\n",
    "            optimizers,\n",
    "        )\n",
    "\n",
    "        self.total_steps = total_steps\n",
    "        self.model_flops = model_flops\n",
    "        self.start_step = 0\n",
    "\n",
    "    def training_step(self, model, inputs):\n",
    "        if inputs[\"input_ids\"].numel() == 0:\n",
    "            print(\"Inputs: \", inputs)\n",
    "            print(\"Inputs - input_ids\", inputs[\"input_ids\"])\n",
    "            print(\"numel\", inputs[\"input_ids\"].numel())\n",
    "\n",
    "            return torch.tensor(0)\n",
    "        else:\n",
    "            model.train()\n",
    "            inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "            with self.compute_loss_context_manager():\n",
    "                loss = self.compute_loss(model, inputs)\n",
    "\n",
    "            if self.args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "\n",
    "            if self.do_grad_scaling:\n",
    "                self.scaler.scale(loss).backward()\n",
    "            else:\n",
    "                self.accelerator.backward(loss)\n",
    "\n",
    "            return loss.detach() / self.args.gradient_accumulation_steps\n",
    "\n",
    "    def log(self, logs):\n",
    "        \"\"\"\n",
    "        Log `logs` on the various objects watching training.\n",
    "        Subclass and override this method to inject custom behavior.\n",
    "        Args:\n",
    "            logs (`Dict[str, float]`):\n",
    "                The values to log.\n",
    "        \"\"\"\n",
    "        if self.state.epoch is not None:\n",
    "            logs[\"epoch\"] = round(self.state.epoch, 2)\n",
    "\n",
    "        self.update_log_timing(logs)\n",
    "\n",
    "        output = {**logs, **{\"step\": self.state.global_step}}\n",
    "        self.update_history(output)\n",
    "\n",
    "        logger.debug(\"Step (\" + str(self.state.global_step) + \") Logs: \" + str(logs))\n",
    "        self.control = self.callback_handler.on_log(\n",
    "            self.args, self.state, self.control, logs\n",
    "        )\n",
    "\n",
    "    def update_log_timing(self, logs):\n",
    "        if len(self.state.log_history) == 0:\n",
    "            self.start_time = time.time()\n",
    "            logs[\"iter_time\"] = 0.0\n",
    "            logs[\"flops\"] = 0.0\n",
    "            logs[\"remaining_time\"] = 0.0\n",
    "            self.start_step = self.state.global_step\n",
    "        elif self.state.global_step > self.start_step:\n",
    "            logs[\"iter_time\"] = (time.time() - self.start_time) / (\n",
    "                self.state.global_step - self.start_step\n",
    "            )\n",
    "            logs[\"flops\"] = self.model_flops / logs[\"iter_time\"]\n",
    "            logs[\"remaining_time\"] = (self.total_steps - self.state.global_step) * logs[\n",
    "                \"iter_time\"\n",
    "            ]\n",
    "\n",
    "    def update_history(self, output):\n",
    "        if \"eval_loss\" in output:\n",
    "            return\n",
    "        if len(self.state.log_history) > 0:\n",
    "            smoothing_window = 100\n",
    "            p = 1.0 / smoothing_window\n",
    "            if \"loss\" in output:\n",
    "                output[\"loss\"] = output[\"loss\"] * p + self.state.log_history[-1][\n",
    "                    \"loss\"\n",
    "                ] * (1.0 - p)\n",
    "        self.state.log_history.append(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hnlZjy_lc32v"
   },
   "outputs": [],
   "source": [
    "max_steps = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8IDgiEwzdFni"
   },
   "outputs": [],
   "source": [
    "trained_model_name = f\"lamini_docs_{max_steps}_steps\"\n",
    "output_dir = trained_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4R1gh4vfdFjN"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "\n",
    "    # Learning rate\n",
    "    learning_rate=1.0e-5,\n",
    "\n",
    "    # Number of training epochs\n",
    "    num_train_epochs=1,\n",
    "\n",
    "    # Max steps to train for (each step is a batch of data)\n",
    "    # Overrides num_train_epochs, if not -1\n",
    "    max_steps=max_steps,\n",
    "\n",
    "    # Batch size for training\n",
    "    per_device_train_batch_size=1,\n",
    "\n",
    "    # Directory to save model checkpoints\n",
    "    output_dir=output_dir,\n",
    "\n",
    "    # Other arguments\n",
    "    overwrite_output_dir=False, # Overwrite the content of the output directory\n",
    "    disable_tqdm=False, # Disable progress bars\n",
    "    eval_steps=120, # Number of update steps between two evaluations\n",
    "    save_steps=120, # After # steps model is saved\n",
    "    warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
    "    per_device_eval_batch_size=1, # Batch size for evaluation\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    optim=\"adafactor\",\n",
    "    gradient_accumulation_steps = 4,\n",
    "    gradient_checkpointing=False,\n",
    "\n",
    "    # Parameters for early stopping\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qefhS6SUdFcx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 512)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
      ")\n",
      "Memory footprint 0.30687256 GB\n",
      "Flops 2195.667812352 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "model_flops = (\n",
    "    base_model.floating_point_ops(\n",
    "        {\n",
    "            \"input_ids\": torch.zeros(\n",
    "                (1, training_config[\"model\"][\"max_length\"])\n",
    "            )\n",
    "        }\n",
    "    ) * training_args.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "print(base_model)\n",
    "print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
    "print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wvcOw_YXdFS0"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    model_flops=model_flops,\n",
    "    total_steps=max_steps,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Hk1MC6XpdLRe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:02, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "NKHaskC-dLO3"
   },
   "outputs": [],
   "source": [
    "save_dir = f'{output_dir}/final'\n",
    "\n",
    "# trainer.save_model(save_dir)\n",
    "# print(\"Saved model to:\", save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDh2zKHydUta"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "hrrKYsAadLI-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)\n",
    "finetuned_slightly_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "KU13gHVudLL7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question input (test): Can Lamini generate technical documentation or user manuals for software projects?\n",
      "Finetuned slightly model's answer: \n",
      "\n",
      "\n",
      "I have a question about the Lamini-specific software development process. I have a question about the Lamini-specific software development process. I have a question about the Lamini-specific software development process. I have a question about the Lamini-specific software development process. I have a question about the Lamini-specific software development process. I have a question about the Lamin\n"
     ]
    }
   ],
   "source": [
    "test_question = test_dataset[0]['question']\n",
    "print(\"Question input (test):\", test_question)\n",
    "\n",
    "print(\"Finetuned slightly model's answer: \")\n",
    "print(inference(test_question, finetuned_slightly_model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6H4vGlVJdLE-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target answer output (test): Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.\n"
     ]
    }
   ],
   "source": [
    "test_answer = test_dataset[0]['answer']\n",
    "print(\"Target answer output (test):\", test_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "salMR8pvdp4L"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
